{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# https://d2l.ai/chapter_recurrent-modern/gru.html\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_reg = nn.MSELoss()\n",
    "criterion_class = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            weight=None,\n",
    "            gamma=2.0,\n",
    "            reduction='mean'\n",
    "    ):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = torch.sigmoid(input_tensor)\n",
    "        prob = torch.exp(log_prob)\n",
    "        \n",
    "        term = ((1.0 - prob) ** self.gamma) * log_prob\n",
    "        return F.nll_loss(term, target_tensor)\n",
    "        \n",
    "        #return F.nll_loss(term, target_tensor, weight=self.weight, reduction=self.reduction)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss01(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0, weight=None, size_average=True):\n",
    "        super(FocalLoss01, self).__init__()\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.contiguous().view(input.size(0), input.size(1), -1)\n",
    "            input = input.transpose(1,2)\n",
    "            input = input.contiguous().view(-1, input.size(2)).squeeze()\n",
    "        if target.dim()==4:\n",
    "            target = target.contiguous().view(target.size(0), target.size(1), -1)\n",
    "            target = target.transpose(1,2)\n",
    "            target = target.contiguous().view(-1, target.size(2)).squeeze()\n",
    "        elif target.dim()==3:\n",
    "            target = target.view(-1)\n",
    "        else:\n",
    "            target = target.view(-1, 1)\n",
    "\n",
    "        # compute the negative likelyhood\n",
    "        weight = Variable(self.weight)\n",
    "        logpt = -F.cross_entropy(input, target)\n",
    "        pt = torch.exp(logpt)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = -((1-pt)**self.gamma) * logpt\n",
    "\n",
    "        # averaging (or not) loss\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss02(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0, weight=None, size_average=True):\n",
    "        super(FocalLoss02, self).__init__()\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # if input.dim()>2:\n",
    "        #     input = input.contiguous().view(input.size(0), input.size(1), -1)\n",
    "        #     input = input.transpose(1,2)\n",
    "        #     input = input.contiguous().view(-1, input.size(2)).squeeze()\n",
    "        # if target.dim()==4:\n",
    "        #     target = target.contiguous().view(target.size(0), target.size(1), -1)\n",
    "        #     target = target.transpose(1,2)\n",
    "        #     target = target.contiguous().view(-1, target.size(2)).squeeze()\n",
    "        # elif target.dim()==3:\n",
    "        #     target = target.view(-1)\n",
    "        # else:\n",
    "        #     target = target.view(-1, 1)\n",
    "\n",
    "        input, target = input.unsqueeze(0), target.unsqueeze(0)\n",
    "\n",
    "        # compute.unsqueeze(0)F.cross_entropy(input, target)\n",
    "        logpt = -F.cross_entropy(input, target)\n",
    "        pt = torch.exp(logpt)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = -((1-pt)**self.gamma) * logpt\n",
    "\n",
    "        # averaging (or not) loss\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss03(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0, alpha=1, size_average=True):\n",
    "        super(FocalLoss03, self).__init__()\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        input, target = input.unsqueeze(0), target.unsqueeze(0)\n",
    "\n",
    "        # compute.unsqueeze(0)F.cross_entropy(input, target)\n",
    "        p = 1/(1+np.exp(-input))\n",
    "        loss = -( self.alpha*target + (1-self.alpha)*(1-target) ) * (( 1 - ( target*p + (1-target)*(1-p)) )**self.gamma) * ( target*np.log(p)+(1-target)*np.log(1-p) )\n",
    "\n",
    "        # averaging (or not) loss\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss04(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0, alpha=1, size_average=True):\n",
    "        super(FocalLoss04, self).__init__()\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \n",
    "        input, target = input.unsqueeze(0), target.unsqueeze(0)\n",
    "\n",
    "        loss = -(target * np.log(input) + (1-target) * np.log(1-input)) #BCE\n",
    "\n",
    "        # averaging (or not) loss\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss05(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0, alpha=1, size_average=True):\n",
    "        super(FocalLoss05, self).__init__()\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        input, target = input.unsqueeze(0), target.unsqueeze(0)\n",
    "        input = torch.clamp(input, min = np.exp(-100)) # so we do not log(0)\n",
    "\n",
    "        logpt = (target * np.log(input) + (1-target) * np.log(1-input))\n",
    "        loss = -self.alpha * ((1-np.exp(logpt))**self.gamma) * logpt # for gamma = 0 and alpha = 1 we get the BCELoss\n",
    "\n",
    "        # averaging (or not) loss\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST!!!\n",
    "\n",
    "class FocalLoss05(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0, alpha=1, size_average=True):\n",
    "        super(FocalLoss05, self).__init__()\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        input, target = input.unsqueeze(0), target.unsqueeze(0)\n",
    "        input = torch.clamp(input, min = np.exp(-100)) # so we do not log(0)\n",
    "\n",
    "        logpt = (target * np.log(input) + (1-target) * np.log(1-input))\n",
    "        loss = -self.alpha * ((1-np.exp(logpt))**self.gamma) * logpt # for gamma = 0 and alpha = 1 we get the BCELoss\n",
    "\n",
    "        # averaging (or not) loss\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLossClass(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0, alpha=1, size_average=True):\n",
    "        super(FocalLossClass, self).__init__()\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        input, target = input.unsqueeze(0), target.unsqueeze(0)\n",
    "        input = torch.clamp(input, min = torch.exp(torch.tensor(-100))) # so we do not log(0)\n",
    "\n",
    "        logpt = (target * torch.log(input) + (1-target) * torch.log(1-input))\n",
    "        loss = -self.alpha * ((1-torch.exp(logpt))**self.gamma) * logpt # for gamma = 0 and alpha = 1 we get the BCELoss\n",
    "\n",
    "        # averaging (or not) loss\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #WORKS!\n",
    "# class BalancedFocalLossClass(nn.Module):\n",
    "\n",
    "#     def __init__(self, gamma=0, alpha=0.5, size_average=True):\n",
    "#         super(BalancedFocalLossClass, self).__init__()\n",
    "\n",
    "#         self.gamma = gamma\n",
    "#         self.alpha = alpha\n",
    "#         self.size_average = size_average\n",
    "\n",
    "#     def forward(self, input, target):\n",
    "\n",
    "#         input, target = input.unsqueeze(0), target.unsqueeze(0)\n",
    "#         input = torch.clamp(input, min = torch.exp(torch.tensor(-100))) # so we do not log(0)\n",
    "\n",
    "#         pos = (-self.alpha * (1-input)**self.gamma * torch.log(input))\n",
    "#         neg = (-(1-self.alpha) * (1-1-input)**self.gamma *  torch.log(1-input))\n",
    "\n",
    "#         #pos = ( (1-input)**self.gamma * torch.log(input))\n",
    "#         #neg = ( (input)**self.gamma *  torch.log(1-input))\n",
    "\n",
    "#         loss = -(pos * target + neg * (1-target))\n",
    "\n",
    "#         # averaging (or not) loss\n",
    "#         if self.size_average:\n",
    "#             return loss.mean()\n",
    "#         else:\n",
    "#             return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BalancedFocalLossClass(nn.Module):\n",
    "\n",
    "#     def __init__(self, gamma=0, alpha=0.5, size_average=True):\n",
    "#         super(BalancedFocalLossClass, self).__init__()\n",
    "\n",
    "#         self.gamma = gamma\n",
    "#         self.alpha = alpha\n",
    "#         self.size_average = size_average\n",
    "\n",
    "#     def forward(self, input, target):\n",
    "\n",
    "#         input, target = input.unsqueeze(0), target.unsqueeze(0)\n",
    "        \n",
    "\n",
    "#         #for logits\n",
    "#         # pos = (-self.alpha * (1-F.sigmoid(input))**self.gamma * F.logsigmoid(input))\n",
    "#         # neg = (-(1-self.alpha) * (-F.sigmoid(input))**self.gamma *  F.logsigmoid(1-input))\n",
    "#         # loss = (pos * target + neg * (1-target))\n",
    "\n",
    "#         # for probs\n",
    "#         input = torch.clamp(input, min = torch.exp(torch.tensor(-100))) # so we do not log(0)\n",
    "#         pos = (-self.alpha * (1-input)**self.gamma * torch.log(input))\n",
    "#         neg = (-(1-self.alpha) * (1-1-input)**self.gamma *  torch.log(1-input))\n",
    "#         loss = (pos * target + neg * (1-target))\n",
    "\n",
    "#         # averaging (or not) loss\n",
    "#         if self.size_average:\n",
    "#             return loss.mean()\n",
    "#         else:\n",
    "#             return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class stableBalancedFocalLossClass(nn.Module):\n",
    "\n",
    "#     def __init__(self, gamma=0, alpha=0.5, size_average=True):\n",
    "#         super(stableBalancedFocalLossClass, self).__init__()\n",
    "\n",
    "#         self.gamma = gamma\n",
    "#         self.alpha = alpha\n",
    "#         self.size_average = size_average\n",
    "\n",
    "#     def forward(self, input, target):\n",
    "\n",
    "#         input, target = input.unsqueeze(0), target.unsqueeze(0)\n",
    "\n",
    "#         # fo   r probs\n",
    "#         min_ind = torch.exp(torch.tensor(-100)) # almost 0\n",
    "#         max_ind = torch.tensor(1.0)- torch.exp(torch.tensor(-10)) # almost 1\n",
    "#         input = torch.clamp(input, min = min_ind, max = max_ind) # so we do not log(0)\n",
    "\n",
    "#         pos = (-self.alpha * (1-input)**self.gamma * torch.log(input))\n",
    "#         neg = (-(1-self.alpha) * (1-1-input)**self.gamma *  torch.log(1-input))\n",
    "#         loss = (pos * target + neg * (1-target))\n",
    "\n",
    "#         # Seem pytorch have something like this..\n",
    "#         if loss.mean() >= max_ind:\n",
    "#             floor = 10\n",
    "#         else:\n",
    "#             floor = 1\n",
    "\n",
    "#         loss =  loss * 2 * floor # *2 is just a constant to make it more like BCE\n",
    "\n",
    "#         # averaging (or not) lossinput = torch.clamp(input, min = torch.exp(torch.tensor(-100))) # so we do not log(0)\n",
    "        \n",
    "#         if self.size_average:\n",
    "#             return loss.mean()\n",
    "#         else:\n",
    "#             return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(p):\n",
    "    return (p * np.log(1/p)).sum()\n",
    "\n",
    "def cross_entropy(p, q):\n",
    "    return -(p * np.log(q)).sum() # same as (p * np.log(1/q)).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedFocalLossClass(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0, alpha=0.5, size_average=True):\n",
    "        super(BalancedFocalLossClass, self).__init__()\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        input, target = input.unsqueeze(0), target.unsqueeze(0)\n",
    "        \n",
    "        # Numerical stabilityt pytorhc trick.\n",
    "        log_input = torch.clamp(torch.log(input), -100, 100)\n",
    "        log_input_rev = torch.clamp(torch.log(1-input), -100, 100)\n",
    "\n",
    "        # for probs\n",
    "        pos = (-self.alpha * (1-input)**self.gamma * log_input)\n",
    "        neg = (-(1-self.alpha) * (1-1-input)**self.gamma * log_input_rev)\n",
    "        \n",
    "        loss = (pos * target + neg * (1-target))\n",
    "\n",
    "        # averaging (or not) loss\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_class = nn.BCELoss()\n",
    "\n",
    "x1 = torch.rand([ 3 ,100, 100])\n",
    "x1_b = (x1 > 0.5)*1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0201)\n",
      "tensor(0.3062)\n"
     ]
    }
   ],
   "source": [
    "# criterion_focal01 = FocalLoss01(gamma=1)\n",
    "# criterion_focal02 = FocalLoss02(gamma=1, size_average=False)\n",
    "# criterion_focal03 = FocalLoss03(gamma=1, alpha = 1, size_average=True)\n",
    "\n",
    "# criterion_focal04 = FocalLoss04(gamma=0)\n",
    "# criterion_focal05 = FocalLoss05()\n",
    "\n",
    "#criterion_focal06 = FocalLossClass()\n",
    "criterion_focal07 = BalancedFocalLossClass(gamma=2, alpha=0.05)\n",
    "# criterion_focal07 = stableBalancedFocalLossClass(gamma=0, alpha=0.5)\n",
    "\n",
    "\n",
    "# print(criterion_focal01(x1.unsqueeze(0), x1_b.unsqueeze(0)))\n",
    "# print(criterion_focal02(x1, x1_b))\n",
    "# print(criterion_focal03(x1, x1_b))\n",
    "\n",
    "\n",
    "# print(criterion_focal04(x1, x1_b))\n",
    "\n",
    "# print(criterion_focal05(x1, x1_b))\n",
    "\n",
    "#print(criterion_focal06(x1, x1_b))\n",
    "\n",
    "print(criterion_focal07(x1, x1_b)) \n",
    "\n",
    "\n",
    "print(criterion_class(x1, x1_b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1536)\n",
      "tensor(0.3071)\n"
     ]
    }
   ],
   "source": [
    "criterion_focal07 = BalancedFocalLossClass(gamma=0, alpha=0.5)\n",
    "\n",
    "print(criterion_focal07(x1, x1_b)) \n",
    "print(criterion_class(x1, x1_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(49.8000)\n",
      "tensor(49.8000)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.rand([ 3 ,100, 100]) \n",
    "\n",
    "x2 = torch.zeros([ 3 ,100, 100]) + torch.tensor(1.0) - torch.exp(torch.tensor(0))\n",
    "\n",
    "x1_b = (x1 > 0.5)*1.0 \n",
    "\n",
    "\n",
    "print(criterion_focal07(x2, x1_b)*2) \n",
    "print(criterion_class(x2, x1_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.6996)\n",
      "tensor(5.9996)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.rand([ 3 ,100, 100]) \n",
    "\n",
    "x2 = torch.zeros([ 3 ,100, 100]) + torch.tensor(1.0) - torch.exp(torch.tensor(-12))\n",
    "\n",
    "x1_b = (x1 > 0.5)*1.0 \n",
    "\n",
    "if x2.mean() > torch.tensor(1.0) - torch.exp(torch.tensor(-15)):\n",
    "    floor = 10\n",
    "else:\n",
    "    floor = 1\n",
    "\n",
    "print(criterion_focal07(x2, x1_b) * floor) \n",
    "print(criterion_class(x2, x1_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCELoss()\n"
     ]
    }
   ],
   "source": [
    "print(criterion_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_focal_class01 = BalancedFocalLossClass(gamma=0.0, alpha=0.75)\n",
    "criterion_focal_class02 = BalancedFocalLossClass(gamma=2.0, alpha=0.25)\n",
    "criterion_focal_class03 = BalancedFocalLossClass(gamma=5.0, alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "focal, gamma = 0, alpha = 1 : \t 0.1558169275522232\n",
      "focal, gamma = 1, alpha = 1 : \t 0.019834911450743675\n",
      "focal, gamma = 2, alpha = 1 : \t -0.0006891923840157688\n"
     ]
    }
   ],
   "source": [
    "print(f'focal, gamma = 0, alpha = 1 : \\t {criterion_focal_class01(x1, x1_b)}')\n",
    "print(f'focal, gamma = 1, alpha = 1 : \\t {criterion_focal_class02(x1, x1_b)}')\n",
    "print(f'focal, gamma = 2, alpha = 1 : \\t {criterion_focal_class03(x1, x1_b)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6931)"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.logsigmoid(torch.tensor(.00))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targer\n",
    "IT = torch.tensor(np.stack([np.identity(100), np.identity(100),  np.identity(100)]), dtype= torch.float32)\n",
    "\n",
    "# candidates\n",
    "ZT = torch.zeros([ 3 ,100, 100])\n",
    "RT = torch.rand([ 3 ,100, 100]) \n",
    "\n",
    "# the loss must priorities 0T over ZT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9498)\n",
      "tensor(0.0358)\n"
     ]
    }
   ],
   "source": [
    "criterion = BalancedFocalLossClass(gamma=2, alpha=0.95)\n",
    "\n",
    "\n",
    "print(criterion(ZT, IT)) \n",
    "print(criterion(RT, IT)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss_reg(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0, alpha=1, size_average=True):\n",
    "        super(FocalLoss_reg, self).__init__()\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        input, target = input.unsqueeze(0), target.unsqueeze(0)\n",
    "        input = torch.clamp(input, min = np.exp(-100)) # so we do not log(0)\n",
    "\n",
    "        mse = (target - input)**2\n",
    "\n",
    "        loss = self.alpha * ((1-np.exp(mse))**self.gamma) * mse # for gamma = 0 and alpha = 1 we get the mse\n",
    "\n",
    "        # averaging (or not) loss\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_reg = nn.MSELoss()\n",
    "\n",
    "x1 = torch.rand([ 3 ,100, 100])\n",
    "x1_b = (x1 > 0.5)*1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_focal_reg01 = FocalLoss_reg(gamma=0)\n",
    "criterion_focal_reg02 = FocalLoss_reg(gamma=1)\n",
    "criterion_focal_reg03 = FocalLoss_reg(gamma=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targer\n",
    "IT = torch.tensor(np.stack([np.identity(100), np.identity(100),  np.identity(100)]), dtype= torch.float32)\n",
    "\n",
    "# candidates\n",
    "ZT = torch.zeros([ 3 ,100, 100])\n",
    "OT = torch.zeros([ 3 ,100, 100]) + 1\n",
    "\n",
    "# the loss must priorities 0T over ZT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: \t\t\t\t 0.3157702684402466\n",
      "focal, gamma = 0, alpha = 1 : \t 24.743696212768555\n",
      "focal, gamma = 1, alpha = 1 : \t -inf\n",
      "focal, gamma = 2, alpha = 1 : \t inf\n",
      "\n",
      "\n",
      "focal, gamma = 1, alpha = 1 : \t -0.33513134717941284\n",
      "focal, gamma = 2, alpha = 1 : \t -10.566515922546387\n",
      "focal, gamma = 3, alpha = 1 : \t 586.1552124023438\n"
     ]
    }
   ],
   "source": [
    "print(f'MSE: \\t\\t\\t\\t {criterion_reg(ZT, IT)}')\n",
    "print(f'focal, gamma = 0, alpha = 1 : \\t {criterion_focal_reg01(ZT, IT)}')\n",
    "print(f'focal, gamma = 1, alpha = 1 : \\t {criterion_focal_reg02(ZT, IT)}')\n",
    "print(f'focal, gamma = 2, alpha = 1 : \\t {criterion_focal_reg03(ZT, IT)}')\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(f'focal, gamma = 1, alpha = 1 : \\t {criterion_focal_reg01(OT, IT)}')\n",
    "print(f'focal, gamma = 2, alpha = 1 : \\t {criterion_focal_reg02(OT, IT)}')\n",
    "print(f'focal, gamma = 3, alpha = 1 : \\t {criterion_focal_reg03(OT, IT)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(input, target):\n",
    "    loss = (target - input)**2\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(input, target):\n",
    "    se = target - input\n",
    "    lse = np.exp(se)  #torch.clamp(se, min = np.exp(-100)))\n",
    "    mlse = lse.mean()\n",
    "\n",
    "    return mlse\n",
    "\n",
    "    # return - np.log(loss.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.rand([ 3 ,100, 100]) * 10\n",
    "\n",
    "# targer\n",
    "IT = torch.tensor(np.stack([np.identity(100), np.identity(100),  np.identity(100)]), dtype= torch.float32) * noise\n",
    "\n",
    "# candidates\n",
    "ZT = torch.zeros([ 3 ,100, 100])\n",
    "OT = (torch.zeros([ 3 ,100, 100]) + 1 ) * noise\n",
    "\n",
    "# the loss must priorities 0T over ZT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19.1620)\n",
      "tensor(0.1097)\n"
     ]
    }
   ],
   "source": [
    "print(MSE(ZT, IT))\n",
    "print(MSE(OT, IT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0006)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OT.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss_reg(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0, alpha=1, size_average=True):\n",
    "        super(FocalLoss_reg, self).__init__()\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        input, target = input.unsqueeze(0), target.unsqueeze(0)\n",
    "        #input = torch.clamp(input, min = np.exp(-100)) # could do this for no negatives???\n",
    "\n",
    "        error = target - input\n",
    "        exp_error = np.exp(error)  #torch.clamp(se, min = np.exp(-100)))\n",
    "        loss = exp_error.mean()\n",
    "\n",
    "        # averaging (or not) loss\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShrinkageLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, a=10, c=0.2, size_average=True):\n",
    "        super(ShrinkageLoss, self).__init__()\n",
    "\n",
    "        self.a = a\n",
    "        self.c = c\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        input, target = input.unsqueeze(0), target.unsqueeze(0) \n",
    "\n",
    "        l = torch.abs(target - input)     #F.l1_loss(input, target)\n",
    "\n",
    "        loss = (l**2)/(1 + torch.exp(self.a*(self.c-l)))\n",
    "\n",
    "        # averaging (or not) loss\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_focal_reg = ShrinkageLoss(a=10, c=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0083)\n",
      "tensor(1.0195)\n"
     ]
    }
   ],
   "source": [
    "criterion_focal_reg = ShrinkageLoss(a=1, c=12)\n",
    "\n",
    "print(criterion_focal_reg(ZT, IT))\n",
    "print(criterion_focal_reg(OT, IT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7491.8901)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shannon_entropy(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4593.0488)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(x1_b, x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4593.0488)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(x1_b, x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(223.4796)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy( x1_b, x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msize_average\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "This criterion computes the cross entropy loss between input logits and target.\n",
      "\n",
      "See :class:`~torch.nn.CrossEntropyLoss` for details.\n",
      "\n",
      "Args:\n",
      "    input (Tensor) : Predicted unnormalized logits;\n",
      "        see Shape section below for supported shapes.\n",
      "    target (Tensor) : Ground truth class indices or class probabilities;\n",
      "        see Shape section below for supported shapes.\n",
      "    weight (Tensor, optional): a manual rescaling weight given to each\n",
      "        class. If given, has to be a Tensor of size `C`\n",
      "    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
      "        the losses are averaged over each loss element in the batch. Note that for\n",
      "        some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
      "        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      "        when reduce is ``False``. Default: ``True``\n",
      "    ignore_index (int, optional): Specifies a target value that is ignored\n",
      "        and does not contribute to the input gradient. When :attr:`size_average` is\n",
      "        ``True``, the loss is averaged over non-ignored targets. Note that\n",
      "        :attr:`ignore_index` is only applicable when the target contains class indices.\n",
      "        Default: -100\n",
      "    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      "        losses are averaged or summed over observations for each minibatch depending\n",
      "        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      "        batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      "    reduction (str, optional): Specifies the reduction to apply to the output:\n",
      "        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
      "        ``'mean'``: the sum of the output will be divided by the number of\n",
      "        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
      "        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
      "        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
      "    label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount\n",
      "        of smoothing when computing the loss, where 0.0 means no smoothing. The targets\n",
      "        become a mixture of the original ground truth and a uniform distribution as described in\n",
      "        `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.\n",
      "\n",
      "Shape:\n",
      "    - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n",
      "      in the case of `K`-dimensional loss.\n",
      "    - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with\n",
      "      :math:`K \\geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`.\n",
      "      If containing class probabilities, same shape as the input and each value should be between :math:`[0, 1]`.\n",
      "\n",
      "    where:\n",
      "\n",
      "    .. math::\n",
      "        \\begin{aligned}\n",
      "            C ={} & \\text{number of classes} \\\\\n",
      "            N ={} & \\text{batch size} \\\\\n",
      "        \\end{aligned}\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> # Example of target with class indices\n",
      "    >>> input = torch.randn(3, 5, requires_grad=True)\n",
      "    >>> target = torch.randint(5, (3,), dtype=torch.int64)\n",
      "    >>> loss = F.cross_entropy(input, target)\n",
      "    >>> loss.backward()\n",
      "    >>>\n",
      "    >>> # Example of target with class probabilities\n",
      "    >>> input = torch.randn(3, 5, requires_grad=True)\n",
      "    >>> target = torch.randn(3, 5).softmax(dim=1)\n",
      "    >>> loss = F.cross_entropy(input, target)\n",
      "    >>> loss.backward()\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/torch2023/lib/python3.10/site-packages/torch/nn/functional.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "F.cross_entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1531)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(x1_b * np.log(x1)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1531)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x1_b * (np.log(1/x1))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0988)\n",
      "tensor(0.3070)\n"
     ]
    }
   ],
   "source": [
    "print(criterion_focal02(x1, x1_b))\n",
    "print(criterion_class(x1, x1_b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 3\n",
    "\n",
    "if t == 1 or t == 0:\n",
    "    print('cool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMSLELoss(\n",
       "  (mse): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgoairb: RMSLELoss(\n",
      "  (mse): MSELoss()\n",
      ")\n",
      "..................\n"
     ]
    }
   ],
   "source": [
    "cr = RMSLELoss()\n",
    "print(f'sgoairb: {cr}\\n..................')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.rand([1, 3 ,100, 100])\n",
    "x1_b = (x1 > 0.5)*1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.rand([3,100, 100])\n",
    "x1_b = (x1 > 0.5)*1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_focal = FocalLoss2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FocalLoss2d()\n"
     ]
    }
   ],
   "source": [
    "print(criterion_focal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4365)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion_focal(x1.unsqueeze(0), x1_b.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1051)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.rand([1, 3 ,100, 100])\n",
    "x1_b = (x1 > 0.5)*1.0 \n",
    "\n",
    "sigmoid_focal_loss(x1, x1_b, reduction= 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2466)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.rand([1, 3 ,100, 100])#.type(torch.LongTensor)\n",
    "x1_b = (x1 > 0.5)*1.0 \n",
    "\n",
    "criterion_focal(x1.reshape(-1), x1_b.reshape(-1).type(torch.LongTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3178)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.randn([1, 3 ,100, 100]).float()\n",
    "\n",
    "#x1 = torch.randn([1, 3 ,100, 100]).float()\n",
    "x2 = torch.randn([1, 3 ,100, 100]).float()\n",
    "\n",
    "x3 = torch.randn([1, 3 ,100, 100]).float()\n",
    "x4 = torch.randn([1, 3 ,100, 100]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9961)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x1_ = x1.reshape(-1)\n",
    "# x2_ = x2.reshape(-1)\n",
    "\n",
    "# mask = x1_ > 0\n",
    "\n",
    "# x1_[mask].shape\n",
    "\n",
    "# criterion_reg(x1_, x2_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(losses_list) == list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_list = []\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    x1_ = x1[:,i,:,:].reshape(-1)\n",
    "    x2_ = x2[:,i,:,:].reshape(-1)\n",
    "    mask = (x3[:,i,:,:].reshape(-1) > 0.0001) | (x4[:,i,:,:].reshape(-1) > 0.0001)\n",
    "\n",
    "    losses_list.append(criterion_reg(x1_[mask], x2_[mask]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(2.0036), tensor(1.9821), tensor(1.9411)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    losses_list.append(criterion_class(t1_pred_class[:,i,:,:], t1_binary[:,i,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "losses_list = []\n",
    "\n",
    "for i in range(3):\n",
    "    print(i)\n",
    "    losses_list.append(torch.tensor(1.0))\n",
    "\n",
    "for i in range(3):\n",
    "    losses_list.append(torch.tensor(2.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = torch.stack(losses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[:3].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[-3:].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(2.)\n",
      "tensor(2.)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(losses[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channels = 32\n",
    "D = 16 \n",
    "\n",
    "\n",
    "X = torch.rand(1, 3, D, D)\n",
    "H = torch.rand(1, hidden_channels, D, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m H\u001b[39m+\u001b[39;49mX\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "H+X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = torch.nn.Conv2d(32,32, 3, padding= 'same')\n",
    "c2 = torch.nn.Conv2d(32,32, 3, padding= 'same', bias = False)\n",
    "\n",
    "c3 = torch.nn.Conv2d(32,32, 3, padding= 'same')\n",
    "c4 = torch.nn.Conv2d(32,32, 3, padding= 'same', bias = False)\n",
    "\n",
    "c5 = torch.nn.Conv2d(32,32, 3, padding= 'same')\n",
    "c6 = torch.nn.Conv2d(32,32, 3, padding= 'same', bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = torch.sigmoid(c1(X) + c2(H))\n",
    "\n",
    "R = torch.sigmoid(c3(X) + c4(H))\n",
    "\n",
    "H_tilde = torch.tanh(c5(X) + c6(torch.mul(R,H)))\n",
    "\n",
    "H = torch.mul(torch.mul(Z,H) + (1 - Z), H_tilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 16, 16])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 48, 3, 180, 180])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol = np.zeros([1,48,3,180,180])\n",
    "tens  =torch.tensor(vol)\n",
    "\n",
    "torch.stack((tens,tens)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = np.zeros([1,48,3,180,180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol[:,:,0,:,:] = 1\n",
    "vol[:,:,1,:,:] = 2\n",
    "vol[:,:,2,:,:] = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 48, 3, 180, 180)\n",
      "1.0\n",
      "2.0\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "print(vol.shape)\n",
    "print(vol[:,:,0,:,:].mean())\n",
    "print(vol[:,:,1,:,:].mean())\n",
    "print(vol[:,:,2,:,:].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 144, 180, 180)\n"
     ]
    }
   ],
   "source": [
    "N = vol.shape[0] # batch size. Always 1\n",
    "C = vol.shape[1] # months\n",
    "D = vol.shape[2] # features\n",
    "H = vol.shape[3] # height\n",
    "W = vol.shape[4] # width\n",
    "\n",
    "vol2 = vol.reshape(N, C*D, H, W)\n",
    "print(vol2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer = transforms.Compose([transforms.RandomRotation((0,360)), transforms.RandomHorizontalFlip(p=0.5), transforms.RandomVerticalFlip(p=0.5)])\n",
    "transformer = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5), transforms.RandomVerticalFlip(p=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data augmentation (can be turned of for final experiments)        \n",
    "vol2 = transformer(torch.tensor(vol2)) # rotations and flips # skip for now... '''''''''''''''''''''''''''''''''''''''''''''''''''''' bug only take 4 dims.. could just squezze the batrhc dom and then give it again afterwards?#train_tensor = train_tensor.reshape(N, C, D, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 48, 3, 180, 180])\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(2., dtype=torch.float64)\n",
      "tensor(3., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "vol3 = vol2.reshape(N,C,D,H,W)\n",
    "print(vol3.shape)\n",
    "print(vol3[:,:,0,:,:].mean())\n",
    "print(vol3[:,:,1,:,:].mean())\n",
    "print(vol3[:,:,2,:,:].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(vol3 == vol).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6e07914b3d36b3c0ae84e5e97633abec307a3dfe696e94a6042eabf44e48503"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
