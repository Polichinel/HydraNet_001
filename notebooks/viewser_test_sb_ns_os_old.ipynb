{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning\n",
    "\n",
    "The system makes use of three different (train, predict) partitioning schemes, used at different points in the process. The three are currently given in the following form:\n",
    "\n",
    "calib_partitioner_dict = {\"train\":(121,396),\"predict\":(397,444)}\n",
    "\n",
    "test_partitioner_dict = {\"train\":(121,444),\"predict\":(445,492)}\n",
    "\n",
    "future_partitioner_dict = {\"train\":(121,492),\"predict\":(493,504)}\n",
    "\n",
    "The numbers are month numbers, defined as number of months since December 1979. 444, then, is December 2015, and 504 December 2021. We change the periods for each of these partitions regularly, typically after an annual update of the most important input data is ready, and, in the case of the future partition, for each monthly update. For calibration and testing purposes, the 'predict' parts are 48 consecutive months.\n",
    "\n",
    "The number of schemes is due to how VIEWS is training models in multiple stages -- using all data available while avoiding data leakage for the purposes of hyper-parameter tuning (short-hand for procedures for calibration, training model weights, and other hyper-parameter settings), testing of model performance, and future prediction.\n",
    "\n",
    "The 'calib_partitioner' is used for hyper-parameter tuning for test purposes, the 'test_partitioner' for model training for test purposes as well as for hyper-parameter tuning for future predictions, and the 'future_partitioner' for model training for future predictions. In other words, when we do a run to test the performance of the model system, we do the following:\n",
    "\n",
    "- train the constituent models on the 'train' part of the calib_partitioner, predict on the 'predict' part of the same, train the hyper-parameters and ensemble weights on these predictions against the actuals in the 'predict' part of this partitioner\n",
    "- retrain the constituent models with optimized hyper-parameters using the 'train' part of the test_partitioner, predict using the pre-trained ensemble weights for the 'predict' part of the test_partitioner, and evaluate these predictions against the actuals of this predict period.\n",
    "\n",
    "For future predictions, we retrain all models to make use of all available data:\n",
    "\n",
    "- train the constituent models on the 'train' part of the test_partitioner, predict on the 'predict' part of the same, train the hyper-parameters and ensemble weights on these predictions against the actuals in the 'predict' part of this partitioner\n",
    "- retrain the constituent models with optimized hyper-parameters using the 'train' part of the future_partitioner, predict using the pre-trained ensemble weights for the 'predict' part of the future_partitioner. As true future predictions evaluation is not possible until at a later date.\n",
    "\n",
    "In practice, the use of the partitioner is in fact a bit more involved than this, see the section on 'stepshifting' below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use viewser env\n",
    "\n",
    "from viewser import Queryset, Column\n",
    "#from ingester3.extensions import *\n",
    "#from ingester3.DBWriter import DBWriter\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "#import geopandas as gpd\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running version 1.9.1 which is consistent with the documentation\n"
     ]
    }
   ],
   "source": [
    "from pkg_resources import get_distribution\n",
    "installed_version = get_distribution('ingester3').version\n",
    "\n",
    "if installed_version >= '1.8.1':\n",
    "    print (f\"You are running version {installed_version} which is consistent with the documentation\")\n",
    "else:\n",
    "    print (f\"\"\"\n",
    "You are running an obsolete version ({installed_version}). Run:\n",
    "pip install ingester3 --upgrade \n",
    "to upgrade\"\"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a list of available transforms\n",
    "#!viewser transforms list\n",
    "\n",
    "# Show a list of available transforms\n",
    "#!viewser tables list\n",
    "\n",
    "#!viewser tables show priogrid_month\n",
    "\n",
    "#!viewser tables show priogrid_year\n",
    "\n",
    "#!viewser tables show country_month\n",
    "\n",
    "#!viewser tables show country_year\n",
    "\n",
    "#!viewser tables show ged2_pgm\n",
    "\n",
    "#!viewser tables show month\n",
    "\n",
    "#!viewser queryset list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  > Ged2_pgm\n",
      "  -------------------------------------------------------\n",
      "  | name                    | path                    |\n",
      "  |:------------------------|:------------------------|\n",
      "  | priogrid_month_id       | priogrid_month_id       |\n",
      "  | ged_sb_best_sum_nokgi   | ged_sb_best_sum_nokgi   |\n",
      "  | ged_ns_best_sum_nokgi   | ged_ns_best_sum_nokgi   |\n",
      "  | ged_os_best_sum_nokgi   | ged_os_best_sum_nokgi   |\n",
      "  | ged_sb_best_count_nokgi | ged_sb_best_count_nokgi |\n",
      "  | ged_ns_best_count_nokgi | ged_ns_best_count_nokgi |\n",
      "  | ged_os_best_count_nokgi | ged_os_best_count_nokgi |\n",
      "  | ged_sb_high_sum_nokgi   | ged_sb_high_sum_nokgi   |\n",
      "  | ged_ns_high_sum_nokgi   | ged_ns_high_sum_nokgi   |\n",
      "  | ged_os_high_sum_nokgi   | ged_os_high_sum_nokgi   |\n",
      "  | ged_sb_high_count_nokgi | ged_sb_high_count_nokgi |\n",
      "  | ged_ns_high_count_nokgi | ged_ns_high_count_nokgi |\n",
      "  | ged_os_high_count_nokgi | ged_os_high_count_nokgi |\n",
      "  | test                    | test                    |\n",
      "  \n",
      "  -------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!viewser tables show ged2_pgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_views_date(location, partitioner_dict):\n",
    "\n",
    "    path_views_data = location + '/views_data.pkl'\n",
    "\n",
    "    if os.path.isfile(path_views_data) == True:\n",
    "\n",
    "        print('File already downloaded')\n",
    "        df = pd.read_pickle(path_views_data)\n",
    "        \n",
    "    else:\n",
    "        print('Beginning file download through viewser...')\n",
    "\n",
    "        # queryset_base = (Queryset(\"simon_tests\", \"priogrid_month\")\n",
    "        #     .with_column(Column(\"sb_best_count_pgm\", from_table = \"ged2_pgm\", from_column = \"ged_sb_best_count_nokgi\").transform.ops.ln().transform.missing.replace_na()))\n",
    "\n",
    "        queryset_base = (Queryset(\"simon_tests\", \"priogrid_month\")\n",
    "            .with_column(Column(\"ln_sb_best\", from_table = \"ged2_pgm\", from_column = \"ged_sb_best_count_nokgi\").transform.ops.ln().transform.missing.replace_na())\n",
    "            .with_column(Column(\"ln_ns_best\", from_table = \"ged2_pgm\", from_column = \"ged_ns_best_count_nokgi\").transform.ops.ln().transform.missing.replace_na())\n",
    "            .with_column(Column(\"ln_os_best\", from_table = \"ged2_pgm\", from_column = \"ged_os_best_count_nokgi\").transform.ops.ln().transform.missing.replace_na())\n",
    "            .with_column(Column(\"month\", from_table = \"month\", from_column = \"month\"))\n",
    "            .with_column(Column(\"year_id\", from_table = \"country_year\", from_column = \"year_id\"))\n",
    "            .with_column(Column(\"c_id\", from_table = \"country_year\", from_column = \"country_id\")))\n",
    "\n",
    "\n",
    "        # You want high, and you want ns and os.\n",
    "\n",
    "        df = queryset_base.publish().fetch()\n",
    "\n",
    "        df.reset_index(inplace = True)\n",
    "\n",
    "        df.rename(columns={'priogrid_gid': 'pg_id'}, inplace= True)\n",
    "\n",
    "\n",
    "\n",
    "        # df = df[df['month_id'] == 121] # temp sub\n",
    "        # df = df[df['month_id'].isin([121, 122, 123, 124])] # temp sub\n",
    "        \n",
    "        \n",
    "        #month_range = np.arange(partitioner_dict['train'][0],partitioner_dict['predict'][1]+1,1)\n",
    "        # month_range = np.arange(partitioner_dict['train'][0],partitioner_dict['train'][0]+11,1)\n",
    "        month_range = np.arange(partitioner_dict['train'][0],partitioner_dict['train'][0]+21,1)\n",
    "\n",
    "\n",
    "        df = df[df['month_id'].isin(month_range)] # temp sub\n",
    "\n",
    "\n",
    "        #df['lat'] = df.pg.lat # already in PRIO grid as ycoord\n",
    "        #df['lon'] = df.pg.lon # already in PRIO grid as xcoord\n",
    "\n",
    "        #df['month'] = df.pgm.month # See if these can be optained through a query_set\n",
    "        #df['year_id'] = df.pgm.year # See if these can be optained through a query_set\n",
    "        #df['row'] = df.pgm.row # already in PRIO grid as row\n",
    "        #df['col'] = df.pgm.col # already in PRIO grid as col\n",
    "\n",
    "        #df['c_id'] = df.pgy.c_id # See if these can be optained through a query_set\n",
    "\n",
    "        df['in_viewser'] = True\n",
    "        # df['name'] = df.cy.name # No need\n",
    "#        df.to_pickle(path_views_data)\n",
    "\n",
    " #       print('VIEWS data pickled.')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prio_shape(location):\n",
    "\n",
    "    path_prio = location + '/priogrid_shapefiles.zip'\n",
    "\n",
    "    if os.path.isfile(path_prio) == True:\n",
    "        \n",
    "        print('File already downloaded')\n",
    "        prio_grid = gpd.read_file('zip://' + path_prio)\n",
    "\n",
    "        prio_grid =  pd.DataFrame(prio_grid.drop(columns = ['geometry']))\n",
    "\n",
    "    else:\n",
    "        print('Beginning file download PRIO...')\n",
    "        url_prio = 'http://file.prio.no/ReplicationData/PRIO-GRID/priogrid_shapefiles.zip'\n",
    "\n",
    "        urllib.request.urlretrieve(url_prio, path_prio)\n",
    "        prio_grid = gpd.read_file('zip://' + path_prio)\n",
    "\n",
    "        prio_grid =  pd.DataFrame(prio_grid.drop(columns = ['geometry']))\n",
    "\n",
    "    prio_grid.rename(columns={'gid': 'pg_id'}, inplace= True)\n",
    "\n",
    "    return prio_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly_grid(prio_grid, views_df):\n",
    "\n",
    "    years = [sorted(views_df['year_id'].unique())] * prio_grid.shape[0]\n",
    "    #months = [list(np.arange(1, 13))] * prio_grid.shape[0]\n",
    "\n",
    "    months = [sorted(views_df['month'].unique())] * prio_grid.shape[0] # then you only get one for the test runs# expensive to get these\n",
    "\n",
    "    prio_grid['year_id'] = years\n",
    "    prio_grid['month'] = months\n",
    "\n",
    "    prio_grid = prio_grid.explode('year_id').reset_index(drop=True) \n",
    "    prio_grid = prio_grid.explode('month').reset_index(drop=True) \n",
    "\n",
    "    prio_grid['year_id'] = prio_grid['year_id'].astype(int)\n",
    "    prio_grid['month'] = prio_grid['month'].astype(int)\n",
    "\n",
    "# --------------------\n",
    "\n",
    "    # # Add month_id - Hack - but likely more robust... and prone to error:\n",
    "    # prio_grid['year_month'] = prio_grid['year_id'].astype(str) + '_' + prio_grid['month'].astype(str) \n",
    "\n",
    "    # ts = prio_grid['year_month'].unique()\n",
    "    # n_ts = len(ts)\n",
    "    # month_ids = np.arange(109, n_ts + 109, 1)\n",
    "    # month_id_df = pd.DataFrame({'year_month' : ts, 'month_id': month_ids})\n",
    "    # prio_grid = prio_grid.merge(month_id_df, on = 'year_month', how = 'left')\n",
    "\n",
    "    # prio_grid.drop(columns=['year_month'], inplace= True)\n",
    "\n",
    "# ------------------------\n",
    "\n",
    "    # # Add month_id - Hack, but it works:\n",
    "    #prio_grid['year_month'] = prio_grid['year'].astype(str) + '_' + prio_grid['month'].astype(str)\n",
    "    #month_ids = np.arange(views_df['month_id'].min(), views_df['month_id'].max()+1, 1)\n",
    "    # month_ids = sorted(views_df['month_id'].unique())\n",
    "\n",
    "    # ts = prio_grid['year_month'].unique()\n",
    "    # month_id_df = pd.DataFrame({'year_month' : ts, 'month_id': month_ids})\n",
    "    # prio_grid = prio_grid.merge(month_id_df, on = 'year_month', how = 'left')\n",
    "\n",
    "    # Merge\n",
    "    full_grid = prio_grid.merge(views_df, on = ['pg_id', 'year_id', 'month'], how = 'left')\n",
    "\n",
    "    full_grid.fillna({'ln_sb_best' : 0, 'ln_ns_best' : 0, 'ln_os_best' : 0, 'c_id' : 0, 'in_viewser' : False}, inplace = True) # for c_id 0 is no country\n",
    "\n",
    "    #full_grid[\"month_id\"] = full_grid.groupby([\"year_id\", \"month\"]).transform(lambda x: x.fillna(x.mean(skipna = True)))['month_id'] # I think this is cool, but must check...\n",
    "    full_grid[\"month_id\"] = full_grid.groupby([\"year_id\", \"month\"]).apply(lambda x: x.fillna(x.mean(skipna = True)))['month_id']\n",
    "\n",
    "\n",
    "    # Drop stuff..\n",
    "    full_grid.dropna(inplace=True)\n",
    "    # the point of this is to drop months that were not give and month_id. The PRIO grid explosion makes only whole years, so this removes any excess months\n",
    "\n",
    "    return full_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_grid(grid, views_df):\n",
    "\n",
    "        views_gids = views_df['pg_id'].unique()\n",
    "\n",
    "        # get both dim to 180. Fine since you maxpool(2,2) two time: 180 -> 90 -> 45\n",
    "        # A better number might be 192 since: 192 -> 96 -> 48 -> 24 -> 12 -> 6 -> 3\n",
    "        max_coords = grid[grid['pg_id'].isin(views_gids)][['xcoord', 'ycoord']].max() + (1,1) \n",
    "        min_coords = grid[grid['pg_id'].isin(views_gids)][['xcoord', 'ycoord']].min() - (1,0.25) \n",
    "        \n",
    "        # Maks it\n",
    "        mask1 = ((grid['xcoord'] < max_coords[0]) & (grid['xcoord'] > min_coords[0]) & (grid['ycoord'] < max_coords[1]) & (grid['ycoord'] > min_coords[1]))\n",
    "        grid = grid[mask1].copy()\n",
    "\n",
    "        return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_volumn(grid):\n",
    "\n",
    "    # we start with wat we know - but there is no reason not to try with more down til line.\n",
    "\n",
    "    sub_df = grid[['pg_id', 'xcoord', 'ycoord', 'month_id', 'c_id', 'ln_sb_best', 'ln_ns_best', 'ln_os_best']].copy() # remove the everything also the geo col. What about in_viewser?\n",
    "\n",
    "    sub_df_sorted = sub_df.sort_values(['month_id', 'ycoord', 'xcoord'], ascending = [True, False, True])\n",
    "\n",
    "    # try to keep the jazz\n",
    "    #grid_ucdpS = grid_ucdpS[['gid','best', 'low',  'high', 'log_best', 'log_low', 'log_high']].copy() # remove the everything also the geo col. But keep gid. Why not.\n",
    "\n",
    "    x_dim = sub_df['xcoord'].unique().shape[0]\n",
    "    y_dim = sub_df['ycoord'].unique().shape[0]\n",
    "    z_dim = sub_df['month_id'].unique().shape[0]\n",
    "\n",
    "    ucpd_vol = np.array(sub_df_sorted).reshape((z_dim, y_dim, x_dim, -1))\n",
    "\n",
    "    return ucpd_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile():\n",
    "\n",
    "    #location = '/home/projects/ku_00017/data/raw/PRIO'\n",
    "    location = '/home/number_one/Documents/scripts/conflictNet/data/raw'\n",
    "\n",
    "    calib_partitioner_dict = {\"train\":(121,396),\"predict\":(397,444)}\n",
    "\n",
    "    test_partitioner_dict = {\"train\":(121,444),\"predict\":(445,492)}\n",
    "\n",
    "    future_partitioner_dict = {\"train\":(121,492),\"predict\":(493,504)}\n",
    "\n",
    "    df = get_views_date(location, calib_partitioner_dict)\n",
    "    print('done 1')\n",
    "    grid = get_prio_shape(location)\n",
    "    print('done 2')\n",
    "    grid = monthly_grid(grid, df)\n",
    "    print('done 3')\n",
    "    grid = get_sub_grid(grid, df)\n",
    "    print('done 4')\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid['month_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(grid['xcoord'].unique()))\n",
    "print(len(grid['ycoord'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (grid['month_id'] == 121) #& (grid['in_viewser'] == True)\n",
    "\n",
    "x = grid.loc[mask, 'xcoord']\n",
    "y = grid.loc[mask, 'ycoord']\n",
    "\n",
    "s = grid.loc[mask,'ln_os_best']\n",
    "\n",
    "base = world.plot(color='white', edgecolor='black', figsize = [25,25])\n",
    "\n",
    "base.scatter(x, y, s= 6, c = s, marker= '.', cmap = 'rainbow', alpha =0.8, vmin = 0, vmax = grid['ln_sb_best'].max())\n",
    "\n",
    "title = f\"{grid.loc[mask, 'month'].mean()}, {grid.loc[mask, 'year_id'].mean()}\"\n",
    "plt.title(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (grid['month_id'] == 121) #& (grid['in_viewser'] == True)\n",
    "\n",
    "x = grid.loc[mask, 'xcoord']\n",
    "y = grid.loc[mask, 'ycoord']\n",
    "\n",
    "s = grid.loc[mask,'ln_ns_best']\n",
    "\n",
    "base = world.plot(color='white', edgecolor='black', figsize = [25,25])\n",
    "\n",
    "base.scatter(x, y, s= 6, c = s, marker= '.', cmap = 'rainbow', alpha =0.8, vmin = 0, vmax = grid['ln_sb_best'].max())\n",
    "\n",
    "title = f\"{grid.loc[mask, 'month'].mean()}, {grid.loc[mask, 'year_id'].mean()}\"\n",
    "plt.title(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (grid['month_id'] == 121) #& (grid['in_viewser'] == True)\n",
    "\n",
    "x = grid.loc[mask, 'xcoord']\n",
    "y = grid.loc[mask, 'ycoord']\n",
    "\n",
    "s = grid.loc[mask,'ln_sb_best']\n",
    "\n",
    "base = world.plot(color='white', edgecolor='black', figsize = [25,25])\n",
    "\n",
    "base.scatter(x, y, s= 6, c = s, marker= '.', cmap = 'rainbow', alpha =0.8, vmin = 0, vmax = grid['ln_sb_best'].max())\n",
    "\n",
    "title = f\"{grid.loc[mask, 'month'].mean()}, {grid.loc[mask, 'year_id'].mean()}\"\n",
    "plt.title(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make volumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid[grid['month_id'].isna()]['month'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid[grid['month_id'].isna()]['year_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = make_volumn(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3): #vol.shape[0]):\n",
    "\n",
    "    fig, axs  = plt.subplots(1,3, figsize= (15,15))\n",
    "\n",
    "    month = vol[i,:,:,3].mean()\n",
    "\n",
    "    true_sb_masked = np.ma.masked_where((vol[0,:,:,4] == 0), vol[i,:,:,5])\n",
    "    axs[0].imshow(true_sb_masked, cmap = 'rainbow')\n",
    "    axs[0].set_title(f\"sb, {month}\")\n",
    "    \n",
    "    true_ns_masked = np.ma.masked_where((vol[0,:,:,4] == 0), vol[i,:,:,6])\n",
    "    axs[1].imshow(true_ns_masked, cmap = 'rainbow')\n",
    "    axs[1].set_title(f\"ns, {month}\")\n",
    "    \n",
    "    true_os_masked = np.ma.masked_where((vol[0,:,:,4] == 0), vol[i,:,:,7])\n",
    "    axs[2].imshow(true_os_masked, cmap = 'rainbow')\n",
    "    axs[2].set_title(f\"os, {month}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = '/home/number_one/Documents/scripts/conflictNet/data/raw/'\n",
    "file = '/viewser_monthly_vol_calib_sbnsos.pkl'\n",
    "\n",
    "with (open(f'{location + file}', 'rb')) as f:\n",
    "    vol_t = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3): #vol.shape[0]):\n",
    "\n",
    "    fig, axs  = plt.subplots(1,3, figsize= (15,15))\n",
    "\n",
    "    month = vol_t[i,:,:,3].mean()\n",
    "\n",
    "    true_sb_masked = np.ma.masked_where((vol_t[0,:,:,4] == 0), vol_t[i,:,:,5])\n",
    "    axs[0].imshow(true_sb_masked, cmap = 'rainbow')\n",
    "    axs[0].set_title(f\"sb, {month}\")\n",
    "    \n",
    "    true_ns_masked = np.ma.masked_where((vol_t[0,:,:,4] == 0), vol_t[i,:,:,6])\n",
    "    axs[1].imshow(true_ns_masked, cmap = 'rainbow')\n",
    "    axs[1].set_title(f\"ns, {month}\")\n",
    "    \n",
    "    true_os_masked = np.ma.masked_where((vol_t[0,:,:,4] == 0), vol_t[i,:,:,7])\n",
    "    axs[2].imshow(true_os_masked, cmap = 'rainbow')\n",
    "    axs[2].set_title(f\"os, {month}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from computerome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = '/home/number_one/Documents/scripts/conflictNet/data/raw/'\n",
    "file = '/viewser_monthly_vol_furture_sbnsos.pkl'\n",
    "\n",
    "with (open(f'{location + file}', 'rb')) as f:\n",
    "    vol_t2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(11): #vol.shape[0]):\n",
    "\n",
    "    fig, axs  = plt.subplots(1,3, figsize= (15,15))\n",
    "\n",
    "    month = vol_t2[i,:,:,3].mean()\n",
    "\n",
    "    true_sb_masked = np.ma.masked_where((vol_t2[0,:,:,4] == 0), vol_t2[i,:,:,5])\n",
    "    axs[0].imshow(true_sb_masked, cmap = 'rainbow')\n",
    "    axs[0].set_title(f\"sb, {month}\")\n",
    "    \n",
    "    true_ns_masked = np.ma.masked_where((vol_t2[0,:,:,4] == 0), vol_t2[i,:,:,6])\n",
    "    axs[1].imshow(true_ns_masked, cmap = 'rainbow')\n",
    "    axs[1].set_title(f\"ns, {month}\")\n",
    "    \n",
    "    true_os_masked = np.ma.masked_where((vol_t2[0,:,:,4] == 0), vol_t2[i,:,:,7])\n",
    "    axs[2].imshow(true_os_masked, cmap = 'rainbow')\n",
    "    axs[2].set_title(f\"os, {month}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucpd_vol_count_t2 = np.count_nonzero(vol_t2[:,:,:,5], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucpd_vol_count_t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucpd_vol_count_t2 = np.count_nonzero(vol_t2[:,:,:,5:8], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucpd_vol_count_t2.sum(axis=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_window(ucpd_vol, min_events, sample):\n",
    "\n",
    "    \"\"\"Draw/sample a window/patch from the traning tensor.\n",
    "    The dimensions of the windows are HxWxD, \n",
    "    where H=D in {16,32,64} and D is the number of months in the training data.\n",
    "    The windows are constrained to be sampled from an area with some\n",
    "    minimum number of log_best events (min_events).\"\"\"\n",
    "\n",
    "    # with coordinates in vol, log best was 7 #CHANGED FOR VIEWSER\n",
    "#    ucpd_vol_count = np.count_nonzero(ucpd_vol[:,:,:,5], axis = 0) #CHANGED FOR VIEWSER\n",
    "    ucpd_vol_count = np.count_nonzero(ucpd_vol[:,:,:,5:8], axis = 0).sum(axis=2) #for either sb, ns, os\n",
    "\n",
    "\n",
    "    # number of events so >= 1 or > 0 is the same as np.nonzero\n",
    "    min_events_index = np.where(ucpd_vol_count >= min_events) \n",
    "\n",
    "    min_events_row = min_events_index[0]\n",
    "    min_events_col = min_events_index[1]\n",
    "\n",
    "    # it is index... Not lat long.\n",
    "    min_events_indx = [(row, col) for row, col in zip(min_events_row, min_events_col)] \n",
    "    \n",
    "    indx = random.choice(min_events_indx)\n",
    "\n",
    "    dim = np.random.choice([16, 32, 64]) \n",
    "\n",
    "    # if you wnat a random temporal window, it is here.\n",
    "    window_dict = {'lat_indx':indx[0], 'long_indx':indx[1], 'dim' : dim} \n",
    "\n",
    "    return(window_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_train_tensors(ucpd_vol, config, sample):\n",
    "def get_train_tensors(ucpd_vol, sample):\n",
    "\n",
    "      # train_ucpd_vol = ucpd_vol[:-config.time_steps] # not tha last 36 months - these ar for test set\n",
    "    train_ucpd_vol = ucpd_vol[:-48] # not tha last 36 months - these ar for test set\n",
    "\n",
    "\n",
    "    # The lenght of a whole time lime.\n",
    "    seq_len = train_ucpd_vol.shape[0]\n",
    "\n",
    "    # To handle \"edge windows\"\n",
    "    while True:\n",
    "        try:\n",
    "            #window_dict = draw_window(ucpd_vol = ucpd_vol, min_events = config.min_events, sample= sample)\n",
    "            window_dict = draw_window(vol_t2, 22, sample)\n",
    "\n",
    "            min_lat_indx = int(window_dict['lat_indx'] - (window_dict['dim']/2)) \n",
    "            max_lat_indx = int(window_dict['lat_indx'] + (window_dict['dim']/2))\n",
    "            min_long_indx = int(window_dict['long_indx'] - (window_dict['dim']/2))\n",
    "            max_long_indx = int(window_dict['long_indx'] + (window_dict['dim']/2))\n",
    "\n",
    "            #HBL = np.random.randint(7,10,1).item()#CHANGED FOR VIEWSER\n",
    "            #HBL = 5:8 # right now just the log_best #CHANGED FOR VIEWSER\n",
    "\n",
    "            #input_window = train_ucpd_vol[ : , min_lat_indx : max_lat_indx , min_long_indx : max_long_indx, 5:8].reshape(1, seq_len, window_dict['dim'], window_dict['dim'], config.input_channels)\n",
    "            input_window = train_ucpd_vol[ : , min_lat_indx : max_lat_indx , min_long_indx : max_long_indx, 5:8].reshape(1, seq_len, window_dict['dim'], window_dict['dim'], 3)\n",
    "            break\n",
    "\n",
    "        except:\n",
    "            print('Resample edge', end= '\\r') # if you don't like this, simply pad to whol volume from 180x180 to 192x192. But there is a point to a avoide edges that might have wierd artifacts.\n",
    "            continue\n",
    "\n",
    "    # 0 since this is constant across years. 1 dim for batch and one dim for time.\n",
    "    # gids = train_ucpd_vol[0 , min_lat_indx : max_lat_indx , min_long_indx : max_long_indx, 0].reshape(1, 1, window_dict['dim'], window_dict['dim'])\n",
    "    # longitudes = train_ucpd_vol[0 , min_lat_indx : max_lat_indx , min_long_indx : max_long_indx, 1].reshape(1, 1, window_dict['dim'], window_dict['dim'])\n",
    "    # latitudes = train_ucpd_vol[0 , min_lat_indx : max_lat_indx , min_long_indx : max_long_indx, 2].reshape(1, 1, window_dict['dim'], window_dict['dim']) \n",
    "\n",
    "    # gids_tensor = torch.tensor(gids, dtype=torch.int) # must be int. You don't use it any more.\n",
    "    # longitudes_tensor = torch.tensor(longitudes, dtype=torch.float)\n",
    "    # latitudes_tensor = torch.tensor(latitudes, dtype=torch.float)\n",
    "\n",
    "    # meta_tensor_dict = {'gids' : gids_tensor, 'longitudes' : longitudes_tensor, 'latitudes' : latitudes_tensor }\n",
    "    # train_tensor = torch.tensor(input_window).float()\n",
    "\n",
    "    # return(train_tensor, meta_tensor_dict)\n",
    "\n",
    "    return(input_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_window = get_train_tensors(vol_t2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_window.shape)\n",
    "print(input_window.squeeze()[0,:,:,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    fig, axs  = plt.subplots(1,3, figsize= (15,15))\n",
    "\n",
    "    month = vol_t2[i,:,:,3].mean()\n",
    "\n",
    "    true_sb_masked = np.ma.masked_where((vol_t2[0,:,:,4] == 0), vol_t2[i,:,:,5])\n",
    "    axs[0].imshow(true_sb_masked, cmap = 'rainbow')\n",
    "    axs[0].set_title(f\"sb, {month}\")\n",
    "    \n",
    "    true_ns_masked = np.ma.masked_where((vol_t2[0,:,:,4] == 0), vol_t2[i,:,:,6])\n",
    "    axs[1].imshow(true_ns_masked, cmap = 'rainbow')\n",
    "    axs[1].set_title(f\"ns, {month}\")\n",
    "    \n",
    "    true_os_masked = np.ma.masked_where((vol_t2[0,:,:,4] == 0), vol_t2[i,:,:,7])\n",
    "    axs[2].imshow(true_os_masked, cmap = 'rainbow')\n",
    "    axs[2].set_title(f\"os, {month}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_window[:,i,:,:].squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10): #input_window.shape(1):\n",
    "\n",
    "    fig, axs  = plt.subplots(1,3, figsize= (15,15))\n",
    "\n",
    "    axs[0].imshow(input_window.squeeze()[i,:,:,0])\n",
    "    axs[1].imshow(input_window.squeeze()[i,:,:,1])\n",
    "    axs[2].imshow(input_window.squeeze()[i,:,:,2])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = draw_window(vol_t2, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viewser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7eec4667a0e19dfe5b6f29f56c555785b49556fbcf0825d71ca5a02fc9d2fb6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
